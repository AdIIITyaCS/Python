# -*- coding: utf-8 -*-
"""LabTestML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13EQSmd63xjtfZBkTJUxlv8PkMR6xE09B
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Step 1: Randomly assign means m1 and m2
def initialize_means(data, k):
    return data[np.random.choice(range(len(data)), k, replace=False)]

# Step 2: Initialize C1 and C2 by the mean values m1 and m2
def initialize_clusters(means):
    return means.copy()

# Step 3: Calculate Euclidean distances D1i and D2i
def calculate_distances(data, centers):
    distances = []
    for point in data:
        dist = [euclidean_distance(point, center) for center in centers]
        distances.append(dist)
    return np.array(distances)

# Euclidean distance function
def euclidean_distance(point1, point2):
    return np.sqrt(np.sum((point1 - point2) ** 2))

# Step 4: Assign data points to the nearest cluster
def assign_clusters(distances):
    return np.argmin(distances, axis=1)

# Step 5: Calculate the mean (Mi) of datapoints for each cluster
def calculate_new_centers(data, cluster_labels, k):
    new_centers = []
    for i in range(k):
        cluster_points = data[cluster_labels == i]
        if len(cluster_points) > 0:
            new_centers.append(np.mean(cluster_points, axis=0))
        else:
            new_centers.append(centers[i])  # If no points in cluster, keep the old center
    return np.array(new_centers)

# K-Means clustering algorithm
def k_means_clustering(data, k, max_iterations=100):
    # Step 1: Randomly initialize means
    centers = initialize_means(data, k)
    print("Initial Centers:\n", centers)

    for iteration in range(max_iterations):
        # Step 3: Calculate distances
        distances = calculate_distances(data, centers)
        print(f"\nIteration {iteration + 1} Distances:\n", distances)

        # Step 4: Assign clusters
        cluster_labels = assign_clusters(distances)
        print("Cluster Labels:\n", cluster_labels)

        # Step 5: Calculate new centers
        new_centers = calculate_new_centers(data, cluster_labels, k)
        print("New Centers:\n", new_centers)

        # Step 6: Check for convergence
        if np.all(centers == new_centers):
            print("\nConverged! No change in centers.")
            break
        centers = new_centers

    return cluster_labels, centers

# Example 1D Data
data_1d = np.array([2, 4, 10, 12, 3, 20, 30, 11, 25]).reshape(-1, 1)
k = 2

# Run K-Means for 1D data
print("1D Data Clustering:")
cluster_labels_1d, centers_1d = k_means_clustering(data_1d, k)

# Print final clusters for 1D data
print("\nFinal 1D Clusters:")
for i in range(k):
    print(f"Cluster {i + 1}: {data_1d[cluster_labels_1d == i].flatten()}")

# Example 3D Data
data_3d = np.array([[2, 3, 4], [10, 11, 12], [3, 15, 20], [30, 9, 11], [25, 22, 20],
                    [15, 9, 25], [12, 8, 25], [9, 18, 21], [6, 9, 12], [15, 14, 13]])

# Run K-Means for 3D data
print("\n3D Data Clustering:")
cluster_labels_3d, centers_3d = k_means_clustering(data_3d, k)

# Print final clusters for 3D data
print("\nFinal 3D Clusters:")
for i in range(k):
    print(f"Cluster {i + 1}:\n{data_3d[cluster_labels_3d == i]}")

# Visualization of 3D data
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Color coding: Red for Cluster 1, Blue for Cluster 2
colors = ['red', 'blue']
for i in range(k):
    cluster_points = data_3d[cluster_labels_3d == i]
    ax.scatter(cluster_points[:, 0], cluster_points[:, 1], cluster_points[:, 2], c=colors[i], label=f'Cluster {i + 1}')

# Plot cluster centers
ax.scatter(centers_3d[:, 0], centers_3d[:, 1], centers_3d[:, 2], c='green', marker='x', s=100, label='Centers')

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')
ax.legend()
plt.title('K-Means Clustering in 3D Space')
plt.show()



